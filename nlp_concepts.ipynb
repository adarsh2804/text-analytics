{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = f\"Matches the empty string, but only when it is not at the beginning or end of a word. This means that r'py\\B' matches 'python', 'py3', 'py2', but not 'py', 'py.', or 'py!'. \\B is just the opposite of \\b, so word characters in Unicode patterns are Unicode alphanumerics or the underscore? although this can be changed by using the ASCII flag. Word boundaries are determined by the current locale if the LOCALE flag is used.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Matches the empty string, but only when it is not at the beginning or end of a word.',\n",
       " \"This means that r'py\\\\B' matches 'python', 'py3', 'py2', but not 'py', 'py.\",\n",
       " \"', or 'py!'.\",\n",
       " '\\\\B is just the opposite of \\x08, so word characters in Unicode patterns are Unicode alphanumerics or the underscore?',\n",
       " 'although this can be changed by using the ASCII flag.',\n",
       " 'Word boundaries are determined by the current locale if the LOCALE flag is used.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Matches',\n",
       " 'the',\n",
       " 'empty',\n",
       " 'string',\n",
       " ',',\n",
       " 'but',\n",
       " 'only',\n",
       " 'when',\n",
       " 'it',\n",
       " 'is',\n",
       " 'not',\n",
       " 'at',\n",
       " 'the',\n",
       " 'beginning',\n",
       " 'or',\n",
       " 'end',\n",
       " 'of',\n",
       " 'a',\n",
       " 'word',\n",
       " '.',\n",
       " 'This',\n",
       " 'means',\n",
       " 'that',\n",
       " \"r'py\\\\B\",\n",
       " \"'\",\n",
       " 'matches',\n",
       " \"'python\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'py3\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'py2\",\n",
       " \"'\",\n",
       " ',',\n",
       " 'but',\n",
       " 'not',\n",
       " \"'py\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'py\",\n",
       " '.',\n",
       " \"'\",\n",
       " ',',\n",
       " 'or',\n",
       " \"'py\",\n",
       " '!',\n",
       " \"'\",\n",
       " '.',\n",
       " '\\\\B',\n",
       " 'is',\n",
       " 'just',\n",
       " 'the',\n",
       " 'opposite',\n",
       " 'of',\n",
       " '\\x08',\n",
       " ',',\n",
       " 'so',\n",
       " 'word',\n",
       " 'characters',\n",
       " 'in',\n",
       " 'Unicode',\n",
       " 'patterns',\n",
       " 'are',\n",
       " 'Unicode',\n",
       " 'alphanumerics',\n",
       " 'or',\n",
       " 'the',\n",
       " 'underscore',\n",
       " '?',\n",
       " 'although',\n",
       " 'this',\n",
       " 'can',\n",
       " 'be',\n",
       " 'changed',\n",
       " 'by',\n",
       " 'using',\n",
       " 'the',\n",
       " 'ASCII',\n",
       " 'flag',\n",
       " '.',\n",
       " 'Word',\n",
       " 'boundaries',\n",
       " 'are',\n",
       " 'determined',\n",
       " 'by',\n",
       " 'the',\n",
       " 'current',\n",
       " 'locale',\n",
       " 'if',\n",
       " 'the',\n",
       " 'LOCALE',\n",
       " 'flag',\n",
       " 'is',\n",
       " 'used',\n",
       " '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_token = word_tokenize(text)\n",
    "word_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Matches', 'empty', 'string', ',', 'beginning', 'end', 'word', '.', 'This', 'means', \"r'py\\\\B\", \"'\", 'matches', \"'python\", \"'\", ',', \"'py3\", \"'\", ',', \"'py2\", \"'\", ',', \"'py\", \"'\", ',', \"'py\", '.', \"'\", ',', \"'py\", '!', \"'\", '.', '\\\\B', 'opposite', '\\x08', ',', 'word', 'characters', 'Unicode', 'patterns', 'Unicode', 'alphanumerics', 'underscore', '?', 'although', 'changed', 'using', 'ASCII', 'flag', '.', 'Word', 'boundaries', 'determined', 'current', 'locale', 'LOCALE', 'flag', 'used', '.']\n"
     ]
    }
   ],
   "source": [
    "clean_WT = [word for word in word_token if word not in stopwords.words('english')]\n",
    "print(clean_WT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_words = ['earn', 'earned', 'earning', 'earns', 'go', 'gone', 'going', 'history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "earn\n",
      "earn\n",
      "earn\n",
      "earn\n",
      "go\n",
      "gone\n",
      "go\n",
      "histori\n"
     ]
    }
   ],
   "source": [
    "for w in example_words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match\n",
      "empti\n",
      "string\n",
      ",\n",
      "begin\n",
      "end\n",
      "word\n",
      ".\n",
      "thi\n",
      "mean\n",
      "r'py\\b\n",
      "'\n",
      "match\n",
      "'python\n",
      "'\n",
      ",\n",
      "'py3\n",
      "'\n",
      ",\n",
      "'py2\n",
      "'\n",
      ",\n",
      "'pi\n",
      "'\n",
      ",\n",
      "'pi\n",
      ".\n",
      "'\n",
      ",\n",
      "'pi\n",
      "!\n",
      "'\n",
      ".\n",
      "\\B\n",
      "opposit\n",
      "\b\n",
      ",\n",
      "word\n",
      "charact\n",
      "unicod\n",
      "pattern\n",
      "unicod\n",
      "alphanumer\n",
      "underscor\n",
      "?\n",
      "although\n",
      "chang\n",
      "use\n",
      "ascii\n",
      "flag\n",
      ".\n",
      "word\n",
      "boundari\n",
      "determin\n",
      "current\n",
      "local\n",
      "local\n",
      "flag\n",
      "use\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for w in clean_WT:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_words = ['earn', 'earned', 'earning', 'earns', 'go', 'gone', 'going', 'history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "earn\n",
      "earned\n",
      "earning\n",
      "earns\n",
      "go\n",
      "gone\n",
      "going\n",
      "history\n"
     ]
    }
   ],
   "source": [
    "for w in example_words: print(lemmatizer.lemmatize(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches\n",
      "empty\n",
      "string\n",
      ",\n",
      "beginning\n",
      "end\n",
      "word\n",
      ".\n",
      "This\n",
      "mean\n",
      "r'py\\B\n",
      "'\n",
      "match\n",
      "'python\n",
      "'\n",
      ",\n",
      "'py3\n",
      "'\n",
      ",\n",
      "'py2\n",
      "'\n",
      ",\n",
      "'py\n",
      "'\n",
      ",\n",
      "'py\n",
      ".\n",
      "'\n",
      ",\n",
      "'py\n",
      "!\n",
      "'\n",
      ".\n",
      "\\B\n",
      "opposite\n",
      "\b\n",
      ",\n",
      "word\n",
      "character\n",
      "Unicode\n",
      "pattern\n",
      "Unicode\n",
      "alphanumerics\n",
      "underscore\n",
      "?\n",
      "although\n",
      "changed\n",
      "using\n",
      "ASCII\n",
      "flag\n",
      ".\n",
      "Word\n",
      "boundary\n",
      "determined\n",
      "current\n",
      "locale\n",
      "LOCALE\n",
      "flag\n",
      "used\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for w in clean_WT: print(lemmatizer.lemmatize(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = []\n",
    "antonyms = []\n",
    "\n",
    "for syn in wordnet.synsets(\"happy\"):\n",
    "    for i in syn.lemmas():\n",
    "        synonyms.append(i.name())\n",
    "        if i.antonyms():\n",
    "            antonyms.append(i.antonyms()[0].name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['happy', 'felicitous', 'happy', 'glad', 'happy', 'happy', 'well-chosen']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['unhappy']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "antonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Mr. Sam went to the Australia. He was feeling very happy after meeting his family after a long time.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt = nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mr.',\n",
       " 'Sam',\n",
       " 'went',\n",
       " 'to',\n",
       " 'the',\n",
       " 'Australia',\n",
       " '.',\n",
       " 'He',\n",
       " 'was',\n",
       " 'feeling',\n",
       " 'very',\n",
       " 'happy',\n",
       " 'after',\n",
       " 'meeting',\n",
       " 'his',\n",
       " 'family',\n",
       " 'after',\n",
       " 'a',\n",
       " 'long',\n",
       " 'time',\n",
       " '.']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = nltk.pos_tag(wt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mr.', 'NNP'),\n",
       " ('Sam', 'NNP'),\n",
       " ('went', 'VBD'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('Australia', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('He', 'PRP'),\n",
       " ('was', 'VBD'),\n",
       " ('feeling', 'VBG'),\n",
       " ('very', 'RB'),\n",
       " ('happy', 'JJ'),\n",
       " ('after', 'IN'),\n",
       " ('meeting', 'VBG'),\n",
       " ('his', 'PRP$'),\n",
       " ('family', 'NN'),\n",
       " ('after', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('long', 'JJ'),\n",
       " ('time', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "wo_t = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in pt: wo_t.append(i[0]+\"_\"+i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mr._NNP',\n",
       " 'Sam_NNP',\n",
       " 'went_VBD',\n",
       " 'to_TO',\n",
       " 'the_DT',\n",
       " 'Australia_NNP',\n",
       " '._.',\n",
       " 'He_PRP',\n",
       " 'was_VBD',\n",
       " 'feeling_VBG',\n",
       " 'very_RB',\n",
       " 'happy_JJ',\n",
       " 'after_IN',\n",
       " 'meeting_VBG',\n",
       " 'his_PRP$',\n",
       " 'family_NN',\n",
       " 'after_IN',\n",
       " 'a_DT',\n",
       " 'long_JJ',\n",
       " 'time_NN',\n",
       " '._.',\n",
       " 'Mr._NNP',\n",
       " 'Sam_NNP',\n",
       " 'went_VBD',\n",
       " 'to_TO',\n",
       " 'the_DT',\n",
       " 'Australia_NNP',\n",
       " '._.',\n",
       " 'He_PRP',\n",
       " 'was_VBD',\n",
       " 'feeling_VBG',\n",
       " 'very_RB',\n",
       " 'happy_JJ',\n",
       " 'after_IN',\n",
       " 'meeting_VBG',\n",
       " 'his_PRP$',\n",
       " 'family_NN',\n",
       " 'after_IN',\n",
       " 'a_DT',\n",
       " 'long_JJ',\n",
       " 'time_NN',\n",
       " '._.']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wo_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mr._NNP Sam_NNP went_VBD to_TO the_DT Australia_NNP ._. He_PRP was_VBD feeling_VBG very_RB happy_JJ after_IN meeting_VBG his_PRP$ family_NN after_IN a_DT long_JJ time_NN ._.'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(wo_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"New Delhi is capital of India\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt = nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_t = nltk.pos_tag(wt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('New', 'NNP'),\n",
       " ('Delhi', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('capital', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('India', 'NNP')]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.tree.Tree"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nltk.ne_chunk(pos_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (GPE New/NNP Delhi/NNP) is/VBZ capital/NN of/IN (GPE India/NNP))\n"
     ]
    }
   ],
   "source": [
    "print(nltk.ne_chunk(pos_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install svgling\n",
    "\n",
    "#nltk.download('maxent_ne_chunker')\n",
    "\n",
    "#nltk.download('words')\n",
    "\n",
    "# nltk.ne_chunk(pos_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Given a dataset which contains information compiled by the World Health Organization and the United Nations to track factors that affect life expectancy. This dataset records the life expectancy in countries alongwith attributes such as developing status, adult mortality, BMI, population etc. Design a model to predict the life expectancy based on given attributes. This model will help us to know that which factors are affecting the life expectancy in a country. It will be easier for a country to determine the predicting factor which is contributing to lower value of life expectancy. This will help in suggesting a country which area should be given importance in order to efficiently improve the life expectancy of its population.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Given a dataset which contains information compiled by the World Health Organization and the United Nations to track factors that affect life expectancy.',\n",
       " 'This dataset records the life expectancy in countries alongwith attributes such as developing status, adult mortality, BMI, population etc.',\n",
       " 'Design a model to predict the life expectancy based on given attributes.',\n",
       " 'This model will help us to know that which factors are affecting the life expectancy in a country.',\n",
       " 'It will be easier for a country to determine the predicting factor which is contributing to lower value of life expectancy.',\n",
       " 'This will help in suggesting a country which area should be given importance in order to efficiently improve the life expectancy of its population.']"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataset)):\n",
    "    dataset[i] = dataset[i].lower()\n",
    "    dataset[i] = re.sub(r'\\W',\" \", dataset[i])\n",
    "    dataset[i] = re.sub(r'\\s+',\" \", dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['given a dataset which contains information compiled by the world health organization and the united nations to track factors that affect life expectancy ',\n",
       " 'this dataset records the life expectancy in countries alongwith attributes such as developing status adult mortality bmi population etc ',\n",
       " 'design a model to predict the life expectancy based on given attributes ',\n",
       " 'this model will help us to know that which factors are affecting the life expectancy in a country ',\n",
       " 'it will be easier for a country to determine the predicting factor which is contributing to lower value of life expectancy ',\n",
       " 'this will help in suggesting a country which area should be given importance in order to efficiently improve the life expectancy of its population ']"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcnt = {}\n",
    "for data in dataset:\n",
    "    words = nltk.word_tokenize(data)\n",
    "    for word in words:\n",
    "        if word not in wordcnt.keys():\n",
    "            wordcnt[word] = 1\n",
    "        else:\n",
    "            wordcnt[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'given': 3,\n",
       " 'a': 5,\n",
       " 'dataset': 2,\n",
       " 'which': 4,\n",
       " 'contains': 1,\n",
       " 'information': 1,\n",
       " 'compiled': 1,\n",
       " 'by': 1,\n",
       " 'the': 7,\n",
       " 'world': 1,\n",
       " 'health': 1,\n",
       " 'organization': 1,\n",
       " 'and': 1,\n",
       " 'united': 1,\n",
       " 'nations': 1,\n",
       " 'to': 6,\n",
       " 'track': 1,\n",
       " 'factors': 2,\n",
       " 'that': 2,\n",
       " 'affect': 1,\n",
       " 'life': 6,\n",
       " 'expectancy': 6,\n",
       " 'this': 3,\n",
       " 'records': 1,\n",
       " 'in': 4,\n",
       " 'countries': 1,\n",
       " 'alongwith': 1,\n",
       " 'attributes': 2,\n",
       " 'such': 1,\n",
       " 'as': 1,\n",
       " 'developing': 1,\n",
       " 'status': 1,\n",
       " 'adult': 1,\n",
       " 'mortality': 1,\n",
       " 'bmi': 1,\n",
       " 'population': 2,\n",
       " 'etc': 1,\n",
       " 'design': 1,\n",
       " 'model': 2,\n",
       " 'predict': 1,\n",
       " 'based': 1,\n",
       " 'on': 1,\n",
       " 'will': 3,\n",
       " 'help': 2,\n",
       " 'us': 1,\n",
       " 'know': 1,\n",
       " 'are': 1,\n",
       " 'affecting': 1,\n",
       " 'country': 3,\n",
       " 'it': 1,\n",
       " 'be': 2,\n",
       " 'easier': 1,\n",
       " 'for': 1,\n",
       " 'determine': 1,\n",
       " 'predicting': 1,\n",
       " 'factor': 1,\n",
       " 'is': 1,\n",
       " 'contributing': 1,\n",
       " 'lower': 1,\n",
       " 'value': 1,\n",
       " 'of': 2,\n",
       " 'suggesting': 1,\n",
       " 'area': 1,\n",
       " 'should': 1,\n",
       " 'importance': 1,\n",
       " 'order': 1,\n",
       " 'efficiently': 1,\n",
       " 'improve': 1,\n",
       " 'its': 1}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordcnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordcnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('given', 3), ('a', 5), ('dataset', 2), ('which', 4), ('contains', 1), ('information', 1), ('compiled', 1), ('by', 1), ('the', 7), ('world', 1), ('health', 1), ('organization', 1), ('and', 1), ('united', 1), ('nations', 1), ('to', 6), ('track', 1), ('factors', 2), ('that', 2), ('affect', 1), ('life', 6), ('expectancy', 6), ('this', 3), ('records', 1), ('in', 4), ('countries', 1), ('alongwith', 1), ('attributes', 2), ('such', 1), ('as', 1), ('developing', 1), ('status', 1), ('adult', 1), ('mortality', 1), ('bmi', 1), ('population', 2), ('etc', 1), ('design', 1), ('model', 2), ('predict', 1), ('based', 1), ('on', 1), ('will', 3), ('help', 2), ('us', 1), ('know', 1), ('are', 1), ('affecting', 1), ('country', 3), ('it', 1), ('be', 2), ('easier', 1), ('for', 1), ('determine', 1), ('predicting', 1), ('factor', 1), ('is', 1), ('contributing', 1), ('lower', 1), ('value', 1), ('of', 2), ('suggesting', 1), ('area', 1), ('should', 1), ('importance', 1), ('order', 1), ('efficiently', 1), ('improve', 1), ('its', 1)])"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordcnt.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "given 3\n",
      "a 5\n",
      "dataset 2\n",
      "which 4\n",
      "contains 1\n",
      "information 1\n",
      "compiled 1\n",
      "by 1\n",
      "the 7\n",
      "world 1\n",
      "health 1\n",
      "organization 1\n",
      "and 1\n",
      "united 1\n",
      "nations 1\n",
      "to 6\n",
      "track 1\n",
      "factors 2\n",
      "that 2\n",
      "affect 1\n",
      "life 6\n",
      "expectancy 6\n",
      "this 3\n",
      "records 1\n",
      "in 4\n",
      "countries 1\n",
      "alongwith 1\n",
      "attributes 2\n",
      "such 1\n",
      "as 1\n",
      "developing 1\n",
      "status 1\n",
      "adult 1\n",
      "mortality 1\n",
      "bmi 1\n",
      "population 2\n",
      "etc 1\n",
      "design 1\n",
      "model 2\n",
      "predict 1\n",
      "based 1\n",
      "on 1\n",
      "will 3\n",
      "help 2\n",
      "us 1\n",
      "know 1\n",
      "are 1\n",
      "affecting 1\n",
      "country 3\n",
      "it 1\n",
      "be 2\n",
      "easier 1\n",
      "for 1\n",
      "determine 1\n",
      "predicting 1\n",
      "factor 1\n",
      "is 1\n",
      "contributing 1\n",
      "lower 1\n",
      "value 1\n",
      "of 2\n",
      "suggesting 1\n",
      "area 1\n",
      "should 1\n",
      "importance 1\n",
      "order 1\n",
      "efficiently 1\n",
      "improve 1\n",
      "its 1\n"
     ]
    }
   ],
   "source": [
    "for i in wordcnt.items():\n",
    "    print(i[0],i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 7),\n",
       " ('to', 6),\n",
       " ('life', 6),\n",
       " ('expectancy', 6),\n",
       " ('a', 5),\n",
       " ('which', 4),\n",
       " ('in', 4),\n",
       " ('given', 3),\n",
       " ('this', 3),\n",
       " ('will', 3),\n",
       " ('country', 3),\n",
       " ('dataset', 2),\n",
       " ('factors', 2),\n",
       " ('that', 2),\n",
       " ('attributes', 2),\n",
       " ('population', 2),\n",
       " ('model', 2),\n",
       " ('help', 2),\n",
       " ('be', 2),\n",
       " ('of', 2)]"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(wordcnt.items(),key=lambda x:x[1], reverse=True)[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in dataset:\n",
    "    vector = list()\n",
    "    for word in wordcnt:\n",
    "        if word in nltk.word_tokenize(data): vector.append(1)\n",
    "        else: vector.append(0)\n",
    "    x.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'will',\n",
       " 'help',\n",
       " 'in',\n",
       " 'suggesting',\n",
       " 'a',\n",
       " 'country',\n",
       " 'which',\n",
       " 'area',\n",
       " 'should',\n",
       " 'be',\n",
       " 'given',\n",
       " 'importance',\n",
       " 'in',\n",
       " 'order',\n",
       " 'to',\n",
       " 'efficiently',\n",
       " 'improve',\n",
       " 'the',\n",
       " 'life',\n",
       " 'expectancy',\n",
       " 'of',\n",
       " 'its',\n",
       " 'population']"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in dataset:\n",
    "    vector = list()\n",
    "    for word in wordcnt:\n",
    "        count = 0\n",
    "        for i in range(len(nltk.word_tokenize(data))):\n",
    "            if nltk.word_tokenize(data)[i] == word:\n",
    "                count += 1\n",
    "        vector.append(count)\n",
    "    x.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0],\n",
       "       [0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,\n",
       "        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0],\n",
       "       [0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0],\n",
       "       [1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,\n",
       "        1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1]])"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = pd.DataFrame(['he is a good boy', 'she is a good girl', 'boy and girl are good boy girl'], columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,3):\n",
    "    words = sent['text'][i]\n",
    "    words = word_tokenize(words)\n",
    "    texts = [lemmatizer.lemmatize(w) for w in words if w not in stopwords.words('english')]\n",
    "    text = ' '.join(texts)\n",
    "    corpus.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'boy girl good boy girl'"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1],\n",
       "       [0, 1, 1],\n",
       "       [2, 2, 1]], dtype=int64)"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('sample.pdf', 'rb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_reader = PyPDF2.PdfFileReader(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_reader.numPages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_one = pdf_reader.getPage(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyPDF2.pdf.PageObject"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(page_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' A Simple PDF File  This is a small demonstration .pdf file -  just for use in the Virtual Mechanics tutorials. More text. And more  text. And more text. And more text. And more text.  And more text. And more text. And more text. And more text. And more  text. And more text. Boring, zzzzz. And more text. And more text. And  more text. And more text. And more text. And more text. And more text.  And more text. And more text.  And more text. And more text. And more text. And more text. And more  text. And more text. And more text. Even more. Continued on page 2 ...'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_one.extractText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding to PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('sample.pdf', 'rb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_reader = PyPDF2.PdfFileReader(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_page = pdf_reader.getPage(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_writer = PyPDF2.PdfFileWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_writer.addPage(first_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_output = open(\"some_new_pdf_file.pdf\", 'wb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_writer.write(pdf_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('sample.pdf', 'rb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_reader = PyPDF2.PdfFileReader(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''\n",
    "for i in range(pdf_reader.numPages):\n",
    "    text += pdf_reader.getPage(i).extractText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' A Simple PDF File  This is a small demonstration .pdf file -  just for use in the Virtual Mechanics tutorials. More text. And more  text. And more text. And more text. And more text.  And more text. And more text. And more text. And more text. And more  text. And more text. Boring, zzzzz. And more text. And more text. And  more text. And more text. And more text. And more text. And more text.  And more text. And more text.  And more text. And more text. And more text. And more text. And more  text. And more text. And more text. Even more. Continued on page 2 ... Simple PDF File 2  ...continued from page 1. Yet more text. And more text. And more text.  And more text. And more text. And more text. And more text. And more  text. Oh, how boring typing this stuff. But not as boring as watching  paint dry. And more text. And more text. And more text. And more text.  Boring.  More, a little more text. The end, and just as well. '"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataset)):\n",
    "    dataset[i] = dataset[i].lower()\n",
    "    dataset[i] = re.sub(r'\\W',\" \", dataset[i])\n",
    "    dataset[i] = re.sub(r'\\s+',\" \", dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' a simple pdf file this is a small demonstration pdf file just for use in the virtual mechanics tutorials ', 'more text ', 'and more text ', 'and more text ', 'and more text ', 'and more text ', 'and more text ', 'and more text ', 'and more text ', 'and more text ', 'and more text ', 'and more text ', 'boring zzzzz ', 'and more text ', 'and more text ', 'and more text ', 'and more text ', 'and more text ', 'and more text ', 'and more text ', 'and more text ', 'and more text ', 'and more text ', 'and more text ', 'and more text ', 'and more text ', 'and more text ', 'and more text ', 'and more text ', 'even more ', 'continued on page 2 ', 'simple pdf file 2 continued from page 1 ', 'yet more text ', 'and more text ', 'and more text ', 'and more text ', 'and more text ', 'and more text ', 'and more text ', 'and more text ', 'oh how boring typing this stuff ', 'but not as boring as watching paint dry ', 'and more text ', 'and more text ', 'and more text ', 'and more text ', 'boring ', 'more a little more text ', 'the end and just as well ']\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['simple', 'pdf', 'file', 'small', 'demonstration', 'pdf', 'file', 'use', 'virtual', 'mechanics', 'tutorials', 'text', 'text', 'text', 'text', 'text', 'text', 'text', 'text', 'text', 'text', 'text', 'boring', 'zzzzz', 'text', 'text', 'text', 'text', 'text', 'text', 'text', 'text', 'text', 'text', 'text', 'text', 'text', 'text', 'text', 'text', 'even', 'continued', 'page', '2', 'simple', 'pdf', 'file', '2', 'continued', 'page', '1', 'yet', 'text', 'text', 'text', 'text', 'text', 'text', 'text', 'text', 'oh', 'boring', 'typing', 'stuff', 'boring', 'watching', 'paint', 'dry', 'text', 'text', 'text', 'text', 'boring', 'little', 'text', 'end', 'well']\n"
     ]
    }
   ],
   "source": [
    "clean_WT = []\n",
    "for data in dataset:\n",
    "    for word in nltk.word_tokenize(data):\n",
    "        if word not in stopwords.words('english'):\n",
    "            clean_WT.append(word)\n",
    "print(clean_WT)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcnt = {}\n",
    "for data in clean_WT:\n",
    "    words = nltk.word_tokenize(data)\n",
    "    for word in words:\n",
    "        if re.match(\"\\\\b([a-zA-Z0-9])\\\\1\\\\1+\\\\b\", word):\n",
    "            continue\n",
    "        if word not in wordcnt.keys():\n",
    "            wordcnt[word] = 1\n",
    "        else:\n",
    "            wordcnt[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 {'a': 3, 'simple': 2, 'pdf': 3, 'file': 3, 'this': 2, 'is': 1, 'small': 1, 'demonstration': 1, 'just': 2, 'for': 1, 'use': 1, 'in': 1, 'the': 2, 'virtual': 1, 'mechanics': 1, 'tutorials': 1, 'more': 42, 'text': 40, 'and': 38, 'boring': 4, 'even': 1, 'continued': 2, 'on': 1, 'page': 2, '2': 2, 'from': 1, '1': 1, 'yet': 1, 'oh': 1, 'how': 1, 'typing': 1, 'stuff': 1, 'but': 1, 'not': 1, 'as': 3, 'watching': 1, 'paint': 1, 'dry': 1, 'little': 1, 'end': 1, 'well': 1}\n"
     ]
    }
   ],
   "source": [
    "print(len(wordcnt),wordcnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted(wordcnt.items(),key=lambda x:x[1], reverse=True)[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting in array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list()\n",
    "for data in clean_WT:\n",
    "    vector = list()\n",
    "    for word in wordcnt:\n",
    "        count = 0\n",
    "        for i in range(len(nltk.word_tokenize(data))):\n",
    "            if nltk.word_tokenize(data)[i] == word:\n",
    "                count += 1\n",
    "        vector.append(count)\n",
    "    x.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(np.array(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 26 artists>"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.bar(wordcnt.keys(), wordcnt.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = {}\n",
    "for word in wordcnt:\n",
    "    vector1 = list()\n",
    "    for data in clean_WT:\n",
    "        count = 0\n",
    "        for i in nltk.word_tokenize(data):\n",
    "            if i == word:\n",
    "                count += 1\n",
    "        tf_word = round(count/len(nltk.word_tokenize(data)),3)\n",
    "        vector1.append(tf_word)\n",
    "    y[word] = vector1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = {}\n",
    "for word in wordcnt:\n",
    "    doc_count = 0\n",
    "    for data in clean_WT:\n",
    "        if word in nltk.word_tokenize(data):\n",
    "            doc_count += 1\n",
    "    z[word] = round(np.log(len(clean_WT)/doc_count),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "for word in y.keys():\n",
    "    tfidf = []\n",
    "    \n",
    "    for value in y[word]:\n",
    "        score = value*z[word]\n",
    "        tfidf.append(score)\n",
    "        \n",
    "    a.append(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.335895, 0.      , 0.      , ..., 0.      , 0.6398  , 0.      ],\n",
       "       [0.169547, 0.      , 0.      , ..., 0.      , 0.      , 0.      ],\n",
       "       [0.335895, 0.      , 0.      , ..., 0.      , 0.      , 0.      ],\n",
       "       ...,\n",
       "       [0.      , 0.      , 0.      , ..., 0.      , 0.7784  , 0.      ],\n",
       "       [0.      , 0.      , 0.      , ..., 0.      , 0.      , 0.649964],\n",
       "       [0.      , 0.      , 0.      , ..., 0.      , 0.      , 0.649964]])"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "07d792be6b57be6706d062b64748ace5f7e7b33f6afbc78bdfd04c27fd7929d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
