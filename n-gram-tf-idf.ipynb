{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample data 1\n",
    "paragraph =  \"\"\"Thank you all so very much. Thank you to the Academy. \n",
    "               Thank you to all of you in this room. I have to congratulate \n",
    "               the other incredible nominees this year. The Revenant was \n",
    "               the product of the tireless efforts of an unbelievable cast\n",
    "               and crew. First off, to my brother in this endeavor, Mr. Tom \n",
    "               Hardy. Tom, your talent on screen can only be surpassed by \n",
    "               your friendship off screen … thank you for creating a t\n",
    "               ranscendent cinematic experience. Thank you to everybody at \n",
    "               Fox and New Regency … my entire team. I have to thank \n",
    "               everyone from the very onset of my career … To my parents; \n",
    "               none of this would be possible without you. And to my \n",
    "               friends, I love you dearly; you know who you are. And lastly,\n",
    "               I just want to say this: Making The Revenant was about\n",
    "               man's relationship to the natural world. A world that we\n",
    "               collectively felt in 2015 as the hottest year in recorded\n",
    "               history. Our production needed to move to the southern\n",
    "               tip of this planet just to be able to find snow. Climate\n",
    "               change is real, it is happening right now. It is the most\n",
    "               urgent threat facing our entire species, and we need to work\n",
    "               collectively together and stop procrastinating. We need to\n",
    "               support leaders around the world who do not speak for the \n",
    "               big polluters, but who speak for all of humanity, for the\n",
    "               indigenous people of the world, for the billions and \n",
    "               billions of underprivileged people out there who would be\n",
    "               most affected by this. For our children’s children, and \n",
    "               for those people out there whose voices have been drowned\n",
    "               out by the politics of greed. I thank you all for this \n",
    "               amazing award tonight. Let us not take this planet for \n",
    "               granted. I do not take tonight for granted. Thank you so very much.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data 2\n",
    "text = \"\"\"Global warming or climate change has become a worldwide concern. It is gradually developing into an unprecedented environmental crisis evident in melting glaciers, changing weather patterns, rising sea levels, floods, cyclones and droughts. Global warming implies an increase in the average temperature of the Earth due to entrapment of greenhouse gases in the earth’s atmosphere.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF(Term frequency) & IDF (Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF - Term Frequency\n",
    "\n",
    "IDF - Inverse Document Frequency\n",
    "\n",
    "TF-IDF = TF * IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Term Frequency (TF) = Number of occurrences of a word in a document / Number of words in that document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inverse Document (IDF) = log_e (Total Number of documents / Number of documents containing word) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataset)):\n",
    "    dataset[i] = dataset[i].lower()\n",
    "    dataset[i] = re.sub(r'\\W',\" \",dataset[i])\n",
    "    dataset[i] = re.sub(r'\\s+',\" \",dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word count \n",
    "word2count = {}\n",
    "for data in dataset:\n",
    "    words = nltk.word_tokenize(data)\n",
    "    for word in words:\n",
    "        if word not in word2count.keys(): word2count[word] = 1\n",
    "        else: word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF Matix\n",
    "tf_matrix = {}\n",
    "for word in word2count:\n",
    "    doc_tf = list()\n",
    "    for data in dataset: \n",
    "        frequency = 0\n",
    "        for w in nltk.word_tokenize(data):\n",
    "            if word == w: frequency += 1\n",
    "                \n",
    "        tf_word = round(frequency/len(nltk.word_tokenize(data)),3)\n",
    "        doc_tf.append(tf_word)\n",
    "    tf_matrix[word] = doc_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IDF\n",
    "word_idfs = {}\n",
    "for word in word2count:\n",
    "    doc_count = 0\n",
    "    for data in dataset:\n",
    "        if word in nltk.word_tokenize(data): doc_count +=1\n",
    "    word_idfs[word] = round(np.log(len(dataset)/doc_count),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF\n",
    "\n",
    "tfidf_matrix = []\n",
    "\n",
    "for word in tf_matrix.keys():\n",
    "    tfidf = []\n",
    "    \n",
    "    for value in tf_matrix[word]:\n",
    "        score = value * word_idfs[word]\n",
    "        tfidf.append(score)\n",
    "        \n",
    "    tfidf_matrix.append(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.161155, 0.193   , 0.107115, ..., 0.      , 0.      , 0.193   ],\n",
       "       [0.141449, 0.1694  , 0.188034, ..., 0.      , 0.      , 0.1694  ],\n",
       "       [0.276886, 0.      , 0.184038, ..., 0.      , 0.      , 0.      ],\n",
       "       ...,\n",
       "       [0.      , 0.      , 0.      , ..., 0.380625, 0.      , 0.      ],\n",
       "       [0.      , 0.      , 0.      , ..., 0.293875, 0.336193, 0.      ],\n",
       "       [0.      , 0.      , 0.      , ..., 0.293875, 0.336193, 0.      ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Gram Modelling - Character Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Global warming or climate change has become a worldwide concern. It is gradually developing into an unprecedented environmental crisis evident in melting glaciers, changing weather patterns, rising sea levels, floods, cyclones and droughts. Global warming implies an increase in the average temperature of the Earth due to entrapment of greenhouse gases in the earth’s atmosphere.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model creation\n",
    "for i in range(len(text)-n):\n",
    "    gram = text[i:i+n]\n",
    "    if gram not in ngrams.keys():\n",
    "        ngrams[gram] = list()\n",
    "    ngrams[gram].append(text[i+n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Globa': ['l', 'l'],\n",
       " 'lobal': [' ', ' '],\n",
       " 'obal ': ['w', 'w'],\n",
       " 'bal w': ['a', 'a'],\n",
       " 'al wa': ['r', 'r'],\n",
       " 'l war': ['m', 'm'],\n",
       " ' warm': ['i', 'i'],\n",
       " 'warmi': ['n', 'n'],\n",
       " 'armin': ['g', 'g'],\n",
       " 'rming': [' ', ' '],\n",
       " 'ming ': ['o', 'i'],\n",
       " 'ing o': ['r'],\n",
       " 'ng or': [' '],\n",
       " 'g or ': ['c'],\n",
       " ' or c': ['l'],\n",
       " 'or cl': ['i'],\n",
       " 'r cli': ['m'],\n",
       " ' clim': ['a'],\n",
       " 'clima': ['t'],\n",
       " 'limat': ['e'],\n",
       " 'imate': [' '],\n",
       " 'mate ': ['c'],\n",
       " 'ate c': ['h'],\n",
       " 'te ch': ['a'],\n",
       " 'e cha': ['n'],\n",
       " ' chan': ['g', 'g'],\n",
       " 'chang': ['e', 'i'],\n",
       " 'hange': [' '],\n",
       " 'ange ': ['h'],\n",
       " 'nge h': ['a'],\n",
       " 'ge ha': ['s'],\n",
       " 'e has': [' '],\n",
       " ' has ': ['b'],\n",
       " 'has b': ['e'],\n",
       " 'as be': ['c'],\n",
       " 's bec': ['o'],\n",
       " ' beco': ['m'],\n",
       " 'becom': ['e'],\n",
       " 'ecome': [' '],\n",
       " 'come ': ['a'],\n",
       " 'ome a': [' '],\n",
       " 'me a ': ['w'],\n",
       " 'e a w': ['o'],\n",
       " ' a wo': ['r'],\n",
       " 'a wor': ['l'],\n",
       " ' worl': ['d'],\n",
       " 'world': ['w'],\n",
       " 'orldw': ['i'],\n",
       " 'rldwi': ['d'],\n",
       " 'ldwid': ['e'],\n",
       " 'dwide': [' '],\n",
       " 'wide ': ['c'],\n",
       " 'ide c': ['o'],\n",
       " 'de co': ['n'],\n",
       " 'e con': ['c'],\n",
       " ' conc': ['e'],\n",
       " 'conce': ['r'],\n",
       " 'oncer': ['n'],\n",
       " 'ncern': ['.'],\n",
       " 'cern.': [' '],\n",
       " 'ern. ': ['I'],\n",
       " 'rn. I': ['t'],\n",
       " 'n. It': [' '],\n",
       " '. It ': ['i'],\n",
       " ' It i': ['s'],\n",
       " 'It is': [' '],\n",
       " 't is ': ['g'],\n",
       " ' is g': ['r'],\n",
       " 'is gr': ['a'],\n",
       " 's gra': ['d'],\n",
       " ' grad': ['u'],\n",
       " 'gradu': ['a'],\n",
       " 'radua': ['l'],\n",
       " 'adual': ['l'],\n",
       " 'duall': ['y'],\n",
       " 'ually': [' '],\n",
       " 'ally ': ['d'],\n",
       " 'lly d': ['e'],\n",
       " 'ly de': ['v'],\n",
       " 'y dev': ['e'],\n",
       " ' deve': ['l'],\n",
       " 'devel': ['o'],\n",
       " 'evelo': ['p'],\n",
       " 'velop': ['i'],\n",
       " 'elopi': ['n'],\n",
       " 'lopin': ['g'],\n",
       " 'oping': [' '],\n",
       " 'ping ': ['i'],\n",
       " 'ing i': ['n', 'm'],\n",
       " 'ng in': ['t'],\n",
       " 'g int': ['o'],\n",
       " ' into': [' '],\n",
       " 'into ': ['a'],\n",
       " 'nto a': ['n'],\n",
       " 'to an': [' '],\n",
       " 'o an ': ['u'],\n",
       " ' an u': ['n'],\n",
       " 'an un': ['p'],\n",
       " 'n unp': ['r'],\n",
       " ' unpr': ['e'],\n",
       " 'unpre': ['c'],\n",
       " 'nprec': ['e'],\n",
       " 'prece': ['d'],\n",
       " 'reced': ['e'],\n",
       " 'ecede': ['n'],\n",
       " 'ceden': ['t'],\n",
       " 'edent': ['e'],\n",
       " 'dente': ['d'],\n",
       " 'ented': [' '],\n",
       " 'nted ': ['e'],\n",
       " 'ted e': ['n'],\n",
       " 'ed en': ['v'],\n",
       " 'd env': ['i'],\n",
       " ' envi': ['r'],\n",
       " 'envir': ['o'],\n",
       " 'nviro': ['n'],\n",
       " 'viron': ['m'],\n",
       " 'ironm': ['e'],\n",
       " 'ronme': ['n'],\n",
       " 'onmen': ['t'],\n",
       " 'nment': ['a'],\n",
       " 'menta': ['l'],\n",
       " 'ental': [' '],\n",
       " 'ntal ': ['c'],\n",
       " 'tal c': ['r'],\n",
       " 'al cr': ['i'],\n",
       " 'l cri': ['s'],\n",
       " ' cris': ['i'],\n",
       " 'crisi': ['s'],\n",
       " 'risis': [' '],\n",
       " 'isis ': ['e'],\n",
       " 'sis e': ['v'],\n",
       " 'is ev': ['i'],\n",
       " 's evi': ['d'],\n",
       " ' evid': ['e'],\n",
       " 'evide': ['n'],\n",
       " 'viden': ['t'],\n",
       " 'ident': [' '],\n",
       " 'dent ': ['i'],\n",
       " 'ent i': ['n'],\n",
       " 'nt in': [' '],\n",
       " 't in ': ['m'],\n",
       " ' in m': ['e'],\n",
       " 'in me': ['l'],\n",
       " 'n mel': ['t'],\n",
       " ' melt': ['i'],\n",
       " 'melti': ['n'],\n",
       " 'eltin': ['g'],\n",
       " 'lting': [' '],\n",
       " 'ting ': ['g'],\n",
       " 'ing g': ['l'],\n",
       " 'ng gl': ['a'],\n",
       " 'g gla': ['c'],\n",
       " ' glac': ['i'],\n",
       " 'glaci': ['e'],\n",
       " 'lacie': ['r'],\n",
       " 'acier': ['s'],\n",
       " 'ciers': [','],\n",
       " 'iers,': [' '],\n",
       " 'ers, ': ['c'],\n",
       " 'rs, c': ['h'],\n",
       " 's, ch': ['a'],\n",
       " ', cha': ['n'],\n",
       " 'hangi': ['n'],\n",
       " 'angin': ['g'],\n",
       " 'nging': [' '],\n",
       " 'ging ': ['w'],\n",
       " 'ing w': ['e'],\n",
       " 'ng we': ['a'],\n",
       " 'g wea': ['t'],\n",
       " ' weat': ['h'],\n",
       " 'weath': ['e'],\n",
       " 'eathe': ['r'],\n",
       " 'ather': [' '],\n",
       " 'ther ': ['p'],\n",
       " 'her p': ['a'],\n",
       " 'er pa': ['t'],\n",
       " 'r pat': ['t'],\n",
       " ' patt': ['e'],\n",
       " 'patte': ['r'],\n",
       " 'atter': ['n'],\n",
       " 'ttern': ['s'],\n",
       " 'terns': [','],\n",
       " 'erns,': [' '],\n",
       " 'rns, ': ['r'],\n",
       " 'ns, r': ['i'],\n",
       " 's, ri': ['s'],\n",
       " ', ris': ['i'],\n",
       " ' risi': ['n'],\n",
       " 'risin': ['g'],\n",
       " 'ising': [' '],\n",
       " 'sing ': ['s'],\n",
       " 'ing s': ['e'],\n",
       " 'ng se': ['a'],\n",
       " 'g sea': [' '],\n",
       " ' sea ': ['l'],\n",
       " 'sea l': ['e'],\n",
       " 'ea le': ['v'],\n",
       " 'a lev': ['e'],\n",
       " ' leve': ['l'],\n",
       " 'level': ['s'],\n",
       " 'evels': [','],\n",
       " 'vels,': [' '],\n",
       " 'els, ': ['f'],\n",
       " 'ls, f': ['l'],\n",
       " 's, fl': ['o'],\n",
       " ', flo': ['o'],\n",
       " ' floo': ['d'],\n",
       " 'flood': ['s'],\n",
       " 'loods': [','],\n",
       " 'oods,': [' '],\n",
       " 'ods, ': ['c'],\n",
       " 'ds, c': ['y'],\n",
       " 's, cy': ['c'],\n",
       " ', cyc': ['l'],\n",
       " ' cycl': ['o'],\n",
       " 'cyclo': ['n'],\n",
       " 'yclon': ['e'],\n",
       " 'clone': ['s'],\n",
       " 'lones': [' '],\n",
       " 'ones ': ['a'],\n",
       " 'nes a': ['n'],\n",
       " 'es an': ['d', ' '],\n",
       " 's and': [' '],\n",
       " ' and ': ['d'],\n",
       " 'and d': ['r'],\n",
       " 'nd dr': ['o'],\n",
       " 'd dro': ['u'],\n",
       " ' drou': ['g'],\n",
       " 'droug': ['h'],\n",
       " 'rough': ['t'],\n",
       " 'ought': ['s'],\n",
       " 'ughts': ['.'],\n",
       " 'ghts.': [' '],\n",
       " 'hts. ': ['G'],\n",
       " 'ts. G': ['l'],\n",
       " 's. Gl': ['o'],\n",
       " '. Glo': ['b'],\n",
       " ' Glob': ['a'],\n",
       " 'ng im': ['p'],\n",
       " 'g imp': ['l'],\n",
       " ' impl': ['i'],\n",
       " 'impli': ['e'],\n",
       " 'mplie': ['s'],\n",
       " 'plies': [' '],\n",
       " 'lies ': ['a'],\n",
       " 'ies a': ['n'],\n",
       " 's an ': ['i'],\n",
       " ' an i': ['n'],\n",
       " 'an in': ['c'],\n",
       " 'n inc': ['r'],\n",
       " ' incr': ['e'],\n",
       " 'incre': ['a'],\n",
       " 'ncrea': ['s'],\n",
       " 'creas': ['e'],\n",
       " 'rease': [' '],\n",
       " 'ease ': ['i'],\n",
       " 'ase i': ['n'],\n",
       " 'se in': [' '],\n",
       " 'e in ': ['t'],\n",
       " ' in t': ['h', 'h'],\n",
       " 'in th': ['e', 'e'],\n",
       " 'n the': [' ', ' '],\n",
       " ' the ': ['a', 'E', 'e'],\n",
       " 'the a': ['v'],\n",
       " 'he av': ['e'],\n",
       " 'e ave': ['r'],\n",
       " ' aver': ['a'],\n",
       " 'avera': ['g'],\n",
       " 'verag': ['e'],\n",
       " 'erage': [' '],\n",
       " 'rage ': ['t'],\n",
       " 'age t': ['e'],\n",
       " 'ge te': ['m'],\n",
       " 'e tem': ['p'],\n",
       " ' temp': ['e'],\n",
       " 'tempe': ['r'],\n",
       " 'emper': ['a'],\n",
       " 'mpera': ['t'],\n",
       " 'perat': ['u'],\n",
       " 'eratu': ['r'],\n",
       " 'ratur': ['e'],\n",
       " 'ature': [' '],\n",
       " 'ture ': ['o'],\n",
       " 'ure o': ['f'],\n",
       " 're of': [' '],\n",
       " 'e of ': ['t'],\n",
       " ' of t': ['h'],\n",
       " 'of th': ['e'],\n",
       " 'f the': [' '],\n",
       " 'the E': ['a'],\n",
       " 'he Ea': ['r'],\n",
       " 'e Ear': ['t'],\n",
       " ' Eart': ['h'],\n",
       " 'Earth': [' '],\n",
       " 'arth ': ['d'],\n",
       " 'rth d': ['u'],\n",
       " 'th du': ['e'],\n",
       " 'h due': [' '],\n",
       " ' due ': ['t'],\n",
       " 'due t': ['o'],\n",
       " 'ue to': [' '],\n",
       " 'e to ': ['e'],\n",
       " ' to e': ['n'],\n",
       " 'to en': ['t'],\n",
       " 'o ent': ['r'],\n",
       " ' entr': ['a'],\n",
       " 'entra': ['p'],\n",
       " 'ntrap': ['m'],\n",
       " 'trapm': ['e'],\n",
       " 'rapme': ['n'],\n",
       " 'apmen': ['t'],\n",
       " 'pment': [' '],\n",
       " 'ment ': ['o'],\n",
       " 'ent o': ['f'],\n",
       " 'nt of': [' '],\n",
       " 't of ': ['g'],\n",
       " ' of g': ['r'],\n",
       " 'of gr': ['e'],\n",
       " 'f gre': ['e'],\n",
       " ' gree': ['n'],\n",
       " 'green': ['h'],\n",
       " 'reenh': ['o'],\n",
       " 'eenho': ['u'],\n",
       " 'enhou': ['s'],\n",
       " 'nhous': ['e'],\n",
       " 'house': [' '],\n",
       " 'ouse ': ['g'],\n",
       " 'use g': ['a'],\n",
       " 'se ga': ['s'],\n",
       " 'e gas': ['e'],\n",
       " ' gase': ['s'],\n",
       " 'gases': [' '],\n",
       " 'ases ': ['i'],\n",
       " 'ses i': ['n'],\n",
       " 'es in': [' '],\n",
       " 's in ': ['t'],\n",
       " 'the e': ['a'],\n",
       " 'he ea': ['r'],\n",
       " 'e ear': ['t'],\n",
       " ' eart': ['h'],\n",
       " 'earth': ['’'],\n",
       " 'arth’': ['s'],\n",
       " 'rth’s': [' '],\n",
       " 'th’s ': ['a'],\n",
       " 'h’s a': ['t'],\n",
       " '’s at': ['m'],\n",
       " 's atm': ['o'],\n",
       " ' atmo': ['s'],\n",
       " 'atmos': ['p'],\n",
       " 'tmosp': ['h'],\n",
       " 'mosph': ['e'],\n",
       " 'osphe': ['r'],\n",
       " 'spher': ['e'],\n",
       " 'phere': ['.']}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b', 'b']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams['Glo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model\n",
    "currentgram = text[0:n]\n",
    "result = currentgram\n",
    "for i in range(100):\n",
    "    if currentgram not in ngrams.keys():break\n",
    "    possibilities = ngrams[currentgram]\n",
    "    nextitem = possibilities[random.randrange(len(possibilities))]\n",
    "    result += nextitem\n",
    "    currentgram = result[len(result)-n : len(result)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Global warming or climate change has become a worldwide concern. It is gradually developing into an unprecedented environmental crisis evident in melting glaciers, changing weather patterns, rising sea levels, floods, cyclones and droughts. Global warming implies an increase in the average temperature of the Earth due to entrapment of greenhouse gases in the earth’s atmosphere.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Global warming in melting sea levels, changing into environment of the a worldwidental crisis gradually'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trigram result\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gloods, crisising sea levelonmeltincreento an to aver cris beconcedevern. It of greent of grapmeltin t'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bigram result\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Global warming or climate changing weather patterns, rising sea levels, floods, cyclones an increase in t'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5gram result\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Gram Modelling - Word Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model creation\n",
    "words = nltk.word_tokenize(text)\n",
    "for i in range(len(words)-n):\n",
    "    gram = \" \".join(words[i:i+n])\n",
    "    if gram not in ngrams.keys(): ngrams[gram] = list()\n",
    "    ngrams[gram].append(words[i+n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model\n",
    "currentgram = \" \".join(words[0:n])\n",
    "result = currentgram\n",
    "for i in range(30):\n",
    "    if currentgram not in ngrams.keys():break\n",
    "    possibilities = ngrams[currentgram]\n",
    "    nextitem = possibilities[random.randrange(len(possibilities))]\n",
    "    result += \" \" + nextitem\n",
    "    rwords = nltk.word_tokenize(result)\n",
    "    currentgram = \" \".join(rwords[len(rwords)-n : len(rwords)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Global warming implies an increase in the average temperature of the Earth due to entrapment of greenhouse gases in the earth ’ s atmosphere .'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2-grams\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Global warming or climate change has become a worldwide concern . It is gradually developing into an unprecedented environmental crisis evident in melting glaciers , changing weather patterns , rising sea levels ,'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3-grams\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Global warming or climate change has become a worldwide concern . It is gradually developing into an unprecedented environmental crisis evident in melting glaciers , changing weather patterns , rising sea levels , floods ,'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5-grams\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Global warming or climate change has become a worldwide concern. It is gradually developing into an unprecedented environmental crisis evident in melting glaciers, changing weather patterns, rising sea levels, floods, cyclones and droughts. Global warming implies an increase in the average temperature of the Earth due to entrapment of greenhouse gases in the earth’s atmosphere.'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Semantic Analysis (LSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Data\n",
    "dataset = [\"The amount of polution is increasing day by day\",\n",
    "           \"The concert was just great\",\n",
    "           \"I love to see Gordon Ramsay cook\",\n",
    "           \"Google is introducing a new technology\",\n",
    "           \"AI Robots are examples of great technology present today\",\n",
    "           \"All of us were singing in the concert\",\n",
    "           \"We have launch campaigns to stop pollution and global warming\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the amount of polution is increasing day by day',\n",
       " 'the concert was just great',\n",
       " 'i love to see gordon ramsay cook',\n",
       " 'google is introducing a new technology',\n",
       " 'ai robots are examples of great technology present today',\n",
       " 'all of us were singing in the concert',\n",
       " 'we have launch campaigns to stop pollution and global warming']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = [line.lower() for line in dataset]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF model\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = vectorizer.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 5)\t0.3211483974289089\n",
      "  (0, 9)\t0.6422967948578178\n",
      "  (0, 17)\t0.3211483974289089\n",
      "  (0, 19)\t0.2665807498646048\n",
      "  (0, 26)\t0.3211483974289089\n",
      "  (0, 24)\t0.2278643877752444\n",
      "  (0, 2)\t0.3211483974289089\n",
      "  (0, 34)\t0.2278643877752444\n"
     ]
    }
   ],
   "source": [
    "print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(n_components=4)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create SVD\n",
    "lsa = TruncatedSVD(n_components=4)\n",
    "lsa.fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v column\n",
    "row1 = lsa.components_[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.80743110e-15  7.39829241e-16 -9.71776018e-18  2.17306447e-01\n",
      " -1.86564100e-15  3.90541693e-17  2.17306447e-01  2.45183517e-15\n",
      "  2.83591658e-01  9.68596258e-17 -1.85984416e-15  2.17306447e-01\n",
      " -1.99852854e-15  2.83591658e-01  2.19984735e-16  2.17306447e-01\n",
      "  7.99745874e-16  4.84298129e-17 -1.99852854e-15 -1.63956252e-15\n",
      "  2.13616736e-15  2.17306447e-01  2.83591658e-01 -1.99852854e-15\n",
      " -7.10537403e-16  2.17306447e-01  4.84298129e-17 -1.85984416e-15\n",
      "  2.83591658e-01 -1.85984416e-15  2.83591658e-01  7.99745874e-16\n",
      "  2.17306447e-01 -3.20007555e-15  2.11233236e-15  4.15788444e-01\n",
      " -1.85984416e-15  7.99745874e-16  2.17306447e-01  2.13616736e-15\n",
      "  2.17306447e-01  7.99745874e-16]\n"
     ]
    }
   ],
   "source": [
    "print(row1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Topic 0 :\n",
      "('the', 0.3760982952926374)\n",
      "('concert', 0.34498873923306583)\n",
      "('great', 0.30012402589487364)\n",
      "('of', 0.29579806095266686)\n",
      "('just', 0.2373658292979121)\n",
      "('was', 0.2373658292979121)\n",
      "('day', 0.22892159541504575)\n",
      "('technology', 0.18383834567413393)\n",
      "('all', 0.17824025175628952)\n",
      "('in', 0.17824025175628952)\n",
      "\n",
      " Topic 1 :\n",
      "('to', 0.41578844396700687)\n",
      "('cook', 0.2835916579351071)\n",
      "('gordon', 0.2835916579351071)\n",
      "('love', 0.2835916579351071)\n",
      "('ramsay', 0.2835916579351071)\n",
      "('see', 0.2835916579351071)\n",
      "('and', 0.21730644711292488)\n",
      "('campaigns', 0.21730644711292477)\n",
      "('global', 0.21730644711292477)\n",
      "('have', 0.21730644711292477)\n",
      "\n",
      " Topic 2 :\n",
      "('technology', 0.37791806767144004)\n",
      "('is', 0.34196143806319856)\n",
      "('google', 0.3413969441909747)\n",
      "('introducing', 0.3413969441909747)\n",
      "('new', 0.3413969441909747)\n",
      "('day', 0.14112432680994705)\n",
      "('examples', 0.1138789219537302)\n",
      "('present', 0.1138789219537302)\n",
      "('robots', 0.1138789219537302)\n",
      "('today', 0.1138789219537302)\n",
      "\n",
      " Topic 3 :\n",
      "('day', 0.4654267679041109)\n",
      "('by', 0.23271338395205546)\n",
      "('increasing', 0.23271338395205546)\n",
      "('polution', 0.23271338395205546)\n",
      "('amount', 0.23271338395205537)\n",
      "('is', 0.21264455202450064)\n",
      "('the', 0.12724213180694313)\n",
      "('all', 0.05644664752726518)\n",
      "('in', 0.05644664752726509)\n",
      "('singing', 0.05644664752726509)\n"
     ]
    }
   ],
   "source": [
    "# visulaize\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i, comp in enumerate(lsa.components_):\n",
    "    componentsTerms = zip(terms,comp)\n",
    "    sortedTerms = sorted(componentsTerms, key = lambda x: x[1], reverse=True)\n",
    "    sortedTerms = sortedTerms[:10]\n",
    "    print(\"\\n Topic\", i, \":\")\n",
    "    for term in sortedTerms: print(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visulaize\n",
    "concept_words = {}\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i, comp in enumerate(lsa.components_):\n",
    "    componentsTerms = zip(terms,comp)\n",
    "    sortedTerms = sorted(componentsTerms, key = lambda x: x[1], reverse=True)\n",
    "    sortedTerms = sortedTerms[:10]\n",
    "#    print(\"\\n Topic\", i, \":\")\n",
    "#    for term in sortedTerms: print(term)\n",
    "    concept_words[\"Concept \"+ str(i)] = sortedTerms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concept_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Concept 0 :\n",
      "1.1297395470753957\n",
      "1.4959427190164012\n",
      "0\n",
      "0.18383834567413393\n",
      "0.7797604325216745\n",
      "1.373365598990949\n",
      "0\n",
      "\n",
      " Concept 1 :\n",
      "0\n",
      "0\n",
      "1.8337467336425424\n",
      "0\n",
      "0\n",
      "0\n",
      "1.285014232418706\n",
      "\n",
      " Concept 2 :\n",
      "0.6242100916830926\n",
      "0\n",
      "0\n",
      "1.7440703383075629\n",
      "0.8334337554863608\n",
      "0\n",
      "0\n",
      "\n",
      " Concept 3 :\n",
      "2.2015937554478873\n",
      "0.12724213180694313\n",
      "0\n",
      "0.21264455202450064\n",
      "0\n",
      "0.2965820743887385\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# sentence topics or concepts\n",
    "for key in concept_words.keys():\n",
    "    sentence_scores = list()\n",
    "    for sentence in dataset:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        score = 0\n",
    "        for word in words:\n",
    "            for word_with_score in concept_words[key]:\n",
    "                if word == word_with_score[0] : score += word_with_score[1]\n",
    "        sentence_scores.append(score)\n",
    "        \n",
    "    print(f\"\\n {key} :\")\n",
    "    for sentence_score in sentence_scores : print(sentence_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
