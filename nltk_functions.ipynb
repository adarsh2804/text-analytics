{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to NLP with NLTK\n",
    "\n",
    "NLTK is an excellent library for machine-learning based NLP, written in Python by experts from both academia and industry. Python allows you to create rich data applications rapidly, iterating on hypotheses. The combination of Python + NLTK means that you can easily add language-aware data products to your larger analytical workflows and applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Overview of NLTK\n",
    "\n",
    "NLTK stands for the Natural Language Toolkit and is written by two eminent computational linguists, Steven Bird (Senior Research Associate of the LDC and professor at the University of Melbourne) and Ewan Klein (Professor of Linguistics at Edinburgh University). NTLK provides a combination of natural language corpora, lexical resources, and example grammars with language processing algorithms, methodologies and demonstrations for a very pythonic \"batteries included\" view of Natural Language Processing.\n",
    "\n",
    "As such, NLTK is perfect for researh driven (hypothesis driven) workflows for agile data science. Its suite of libraries includes:\n",
    "\n",
    "    tokenization, stemming, and tagging\n",
    "    chunking and parsing\n",
    "    language modeling\n",
    "    classification and clustering\n",
    "    logical semantics\n",
    "\n",
    "NLTK is a useful pedagogical resource for learning NLP with Python and serves as a starting place for producing production grade code that requires natural language analysis. It is also important to understand what NLTK is not:\n",
    "\n",
    "    Production ready out of the box\n",
    "    Lightweight\n",
    "    Generally applicable\n",
    "    Magic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tokenization\n",
    "\n",
    "Tokenization is the process of dividing the whole text into tokens.\n",
    "\n",
    "It is mainly of two types:\n",
    "\n",
    "    Word Tokenizer (separated by words)\n",
    "    Sentence Tokenizer (separated by sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = f\"\"\" NLTK stands for the Natural Language Toolkit and is written by two eminent computational. linguists, \n",
    "            Steven Bird (Senior Research Associate of the LDC and professor at the University?\n",
    "            of Melbourne) and Ewan Klein (Professor of Linguistics at Edinburgh University). \n",
    "            NTLK provides a combination of natural language corpora, lexical resources, \n",
    "            and example grammars with language processing algorithms, \n",
    "            methodologies and demonstrations for a very pythonic batteries included \n",
    "            view of Natural Language Processing.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' NLTK stands for the Natural Language Toolkit and is written by two eminent computational.',\n",
       " 'linguists, \\n            Steven Bird (Senior Research Associate of the LDC and professor at the University?',\n",
       " 'of Melbourne) and Ewan Klein (Professor of Linguistics at Edinburgh University).',\n",
       " 'NTLK provides a combination of natural language corpora, lexical resources, \\n            and example grammars with language processing algorithms, \\n            methodologies and demonstrations for a very pythonic batteries included \\n            view of Natural Language Processing.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrd_tkn = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLTK',\n",
       " 'stands',\n",
       " 'for',\n",
       " 'the',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Toolkit',\n",
       " 'and',\n",
       " 'is',\n",
       " 'written',\n",
       " 'by',\n",
       " 'two',\n",
       " 'eminent',\n",
       " 'computational',\n",
       " '.',\n",
       " 'linguists',\n",
       " ',',\n",
       " 'Steven',\n",
       " 'Bird',\n",
       " '(',\n",
       " 'Senior',\n",
       " 'Research',\n",
       " 'Associate',\n",
       " 'of',\n",
       " 'the',\n",
       " 'LDC',\n",
       " 'and',\n",
       " 'professor',\n",
       " 'at',\n",
       " 'the',\n",
       " 'University',\n",
       " '?',\n",
       " 'of',\n",
       " 'Melbourne',\n",
       " ')',\n",
       " 'and',\n",
       " 'Ewan',\n",
       " 'Klein',\n",
       " '(',\n",
       " 'Professor',\n",
       " 'of',\n",
       " 'Linguistics',\n",
       " 'at',\n",
       " 'Edinburgh',\n",
       " 'University',\n",
       " ')',\n",
       " '.',\n",
       " 'NTLK',\n",
       " 'provides',\n",
       " 'a',\n",
       " 'combination',\n",
       " 'of',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'corpora',\n",
       " ',',\n",
       " 'lexical',\n",
       " 'resources',\n",
       " ',',\n",
       " 'and',\n",
       " 'example',\n",
       " 'grammars',\n",
       " 'with',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'algorithms',\n",
       " ',',\n",
       " 'methodologies',\n",
       " 'and',\n",
       " 'demonstrations',\n",
       " 'for',\n",
       " 'a',\n",
       " 'very',\n",
       " 'pythonic',\n",
       " 'batteries',\n",
       " 'included',\n",
       " 'view',\n",
       " 'of',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " '.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrd_tkn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Stopwords\n",
    "\n",
    "In general stopwords are the words in any language which does not add much meaning to a sentence. In NLP stopwords are those words which are not important in analyzing the data.\n",
    "Example : he,she,hi,and etc.\n",
    "Our main task is to remove all the stopwords for the text to do any further processing.\n",
    "\n",
    "There are a total of 179 stopwords in English, using NLTK we can see all the stopwords in English.\n",
    "We Just need to import stopwords from the library nltk.corpus ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_wrd_tkns = [word for word in wrd_tkn if word not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLTK', 'stands', 'for', 'the', 'Natural', 'Language', 'Toolkit', 'and', 'is', 'written', 'by', 'two', 'eminent', 'computational', '.', 'linguists', ',', 'Steven', 'Bird', '(', 'Senior', 'Research', 'Associate', 'of', 'the', 'LDC', 'and', 'professor', 'at', 'the', 'University', '?', 'of', 'Melbourne', ')', 'and', 'Ewan', 'Klein', '(', 'Professor', 'of', 'Linguistics', 'at', 'Edinburgh', 'University', ')', '.', 'NTLK', 'provides', 'a', 'combination', 'of', 'natural', 'language', 'corpora', ',', 'lexical', 'resources', ',', 'and', 'example', 'grammars', 'with', 'language', 'processing', 'algorithms', ',', 'methodologies', 'and', 'demonstrations', 'for', 'a', 'very', 'pythonic', 'batteries', 'included', 'view', 'of', 'Natural', 'Language', 'Processing', '.']\n"
     ]
    }
   ],
   "source": [
    "print(wrd_tkn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLTK', 'stands', 'Natural', 'Language', 'Toolkit', 'written', 'two', 'eminent', 'computational', '.', 'linguists', ',', 'Steven', 'Bird', '(', 'Senior', 'Research', 'Associate', 'LDC', 'professor', 'University', '?', 'Melbourne', ')', 'Ewan', 'Klein', '(', 'Professor', 'Linguistics', 'Edinburgh', 'University', ')', '.', 'NTLK', 'provides', 'combination', 'natural', 'language', 'corpora', ',', 'lexical', 'resources', ',', 'example', 'grammars', 'language', 'processing', 'algorithms', ',', 'methodologies', 'demonstrations', 'pythonic', 'batteries', 'included', 'view', 'Natural', 'Language', 'Processing', '.']\n"
     ]
    }
   ],
   "source": [
    "print(clean_wrd_tkns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Stemming\n",
    "\n",
    "Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma.\n",
    "In simple words, we can say that stemming is the process of removing plural and adjectives from the word.\n",
    "Example :\n",
    "gone → go, learning →learn\n",
    "\n",
    "In python, we can implement stemming by using PorterStemmer . we can import it from the library nltk.stem .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "earn\n",
      "earn\n",
      "earn\n",
      "earn\n",
      "go\n",
      "gone\n",
      "go\n",
      "histori\n",
      "zzzzzzz\n"
     ]
    }
   ],
   "source": [
    "example_words = ['earn','earning','earned','earns','go','gone','going','history',\"zzzzzzz\"]\n",
    "\n",
    "for w in example_words: \n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nltk\n",
      "stand\n",
      "natur\n",
      "languag\n",
      "toolkit\n",
      "written\n",
      "two\n",
      "emin\n",
      "comput\n",
      ".\n",
      "linguist\n",
      ",\n",
      "steven\n",
      "bird\n",
      "(\n",
      "senior\n",
      "research\n",
      "associ\n",
      "ldc\n",
      "professor\n",
      "univers\n",
      "?\n",
      "melbourn\n",
      ")\n",
      "ewan\n",
      "klein\n",
      "(\n",
      "professor\n",
      "linguist\n",
      "edinburgh\n",
      "univers\n",
      ")\n",
      ".\n",
      "ntlk\n",
      "provid\n",
      "combin\n",
      "natur\n",
      "languag\n",
      "corpora\n",
      ",\n",
      "lexic\n",
      "resourc\n",
      ",\n",
      "exampl\n",
      "grammar\n",
      "languag\n",
      "process\n",
      "algorithm\n",
      ",\n",
      "methodolog\n",
      "demonstr\n",
      "python\n",
      "batteri\n",
      "includ\n",
      "view\n",
      "natur\n",
      "languag\n",
      "process\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for w in clean_wrd_tkns: \n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Lemmatizing\n",
    "\n",
    "Lemmatization usually refers to doing things properly with the use of vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma.\n",
    "In simple words lemmatization does the same work as stemming, the difference is that lemmatization returns a meaningful word.\n",
    "\n",
    "Example:\n",
    "Stemming\n",
    "history → histori\n",
    "Lemmatizing\n",
    "history → history\n",
    "\n",
    "\n",
    "It is Mostly used when designing chatbots, Q&A bots, text prediction, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmaztier = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "earn\n",
      "earning\n",
      "earned\n",
      "earns\n",
      "go\n",
      "gone\n",
      "going\n",
      "history\n",
      "zzzzzzz\n"
     ]
    }
   ],
   "source": [
    "for w in example_words: print(lemmaztier.lemmatize(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK\n",
      "stand\n",
      "Natural\n",
      "Language\n",
      "Toolkit\n",
      "written\n",
      "two\n",
      "eminent\n",
      "computational\n",
      ".\n",
      "linguist\n",
      ",\n",
      "Steven\n",
      "Bird\n",
      "(\n",
      "Senior\n",
      "Research\n",
      "Associate\n",
      "LDC\n",
      "professor\n",
      "University\n",
      "?\n",
      "Melbourne\n",
      ")\n",
      "Ewan\n",
      "Klein\n",
      "(\n",
      "Professor\n",
      "Linguistics\n",
      "Edinburgh\n",
      "University\n",
      ")\n",
      ".\n",
      "NTLK\n",
      "provides\n",
      "combination\n",
      "natural\n",
      "language\n",
      "corpus\n",
      ",\n",
      "lexical\n",
      "resource\n",
      ",\n",
      "example\n",
      "grammar\n",
      "language\n",
      "processing\n",
      "algorithm\n",
      ",\n",
      "methodology\n",
      "demonstration\n",
      "pythonic\n",
      "battery\n",
      "included\n",
      "view\n",
      "Natural\n",
      "Language\n",
      "Processing\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for w in clean_wrd_tkns: print(lemmaztier.lemmatize(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. WordNet\n",
    "\n",
    "WordNet is the lexical database i.e. dictionary for the English language, specifically designed for natural language processing.\n",
    "We can use wordnet for finding synonyms and antonyms.\n",
    "\n",
    "In python, we can import wordnet from nltk.corpus .\n",
    "Code For Finding Synonym and antonym for a given word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = []\n",
    "antonyms = []\n",
    "\n",
    "for syn in wordnet.synsets(\"happy\"):\n",
    "    for i in syn.lemmas():\n",
    "        synonyms.append(i.name())\n",
    "        if i.antonyms():\n",
    "            antonyms.append(i.antonyms()[0].name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['happy', 'felicitous', 'happy', 'glad', 'happy', 'happy', 'well-chosen']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['unhappy']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "antonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Part of Speech Tagging\n",
    "\n",
    "It is a process of converting a sentence to forms — a list of words, a list of tuples (where each tuple is having a form (word, tag)). The tag in the case is a part-of-speech tag and signifies whether the word is a noun, adjective, verb, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " CC coordinating conjunction\n",
    " CD cardinal digit\n",
    " DT determiner\n",
    " EX existential there (like: “there is” … think of it like “there”)\n",
    " FW foreign word\n",
    " IN preposition/subordinating conjunction\n",
    " JJ adjective ‘big’\n",
    " JJR adjective, comparative ‘bigger’\n",
    " JJS adjective, superlative ‘biggest’\n",
    " LS list marker 1)\n",
    " MD modal could, will\n",
    " NN noun, singular ‘desk’\n",
    " NNS noun plural ‘desks’\n",
    " NNP proper noun, singular ‘Harrison’\n",
    " NNPS proper noun, plural ‘Americans’\n",
    " PDT predeterminer ‘all the kids’\n",
    " POS possessive ending parent’s\n",
    " PRP personal pronoun I, he, she\n",
    " PRP possessive pronoun my, his, hers\n",
    " RB adverb very, silently,\n",
    " RBR adverb, comparative better\n",
    " RBS adverb, superlative best\n",
    " RP particle give up\n",
    " TO to go ‘to’ the store.\n",
    " UH interjection errrrrrrrm\n",
    " VB verb, base form take\n",
    " VBD verb, past tense took\n",
    " VBG verb, gerund/present participle taking\n",
    " VBN verb, past participle taken\n",
    " VBP verb, sing. present, non-3d take\n",
    " VBZ verb, 3rd person sing. present takes\n",
    " WDT wh-determiner which\n",
    " WP wh-pronoun who, what\n",
    " WP possessive wh-pronoun whose\n",
    " WRB wh-abverb where, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Boring, zzzzz. And more text. And more text. And more text.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt  = nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = nltk.pos_tag(wt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "wo_t = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "for wt in pt: wo_t.append(wt[0]+\"_\"+wt[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Boring_NNP ,_, zzzzz_NN ._. And_CC more_JJR text_NN ._. And_CC more_JJR text_NN ._. And_CC more_JJR text_NN ._.'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(wo_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name entity recognition\n",
    "The purpose of named entity recognition is to extract information from unstructured text, especially where it's impractical to have humans read and markup a large number of documents.\n",
    "\n",
    "A named entity can be any type of real-world object or meaningful concept that is assigned a name or a proper name. Typically, named entities can include:\n",
    "\n",
    "    people\n",
    "    organisations\n",
    "    countries\n",
    "    languages\n",
    "    locations\n",
    "    works of art\n",
    "    dates\n",
    "    times\n",
    "    numbers\n",
    "    quantities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Boring, zzzzz. And more text. And more text. And more text.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt = nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_t = nltk.pos_tag(wt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Tree('S', [Tree('GPE', [('Boring', 'NNP')]), (',', ','), ('zzzzz', 'NN'), ('.', '.'), ('And', 'CC'), ('more', 'JJR'), ('text', 'NN'), ('.', '.'), ('And', 'CC'), ('more', 'JJR'), ('text', 'NN'), ('.', '.'), ('And', 'CC'), ('more', 'JJR'), ('text', 'NN'), ('.', '.')])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.ne_chunk(pos_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Bag of words\n",
    "\n",
    "Till now we have understood about tokenizing, stemming, and lemmatizing. all of these are the part of the text cleaning, now after cleaning the text we need to convert the text into some kind of numerical representation called vectors so that we can feed the data to a machine learning model for further processing.\n",
    "\n",
    "For converting the data into vectors we make use of some predefined libraries in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = f\"\"\"Till now we have understood about tokenizing,        stemming, and lemmatizing. \n",
    "            All of these are the part of the text cleaning, now after cleaning the text we need to convert the text into some kind of numerical representation called vectors so that we can feed the data to a machine learning model for further processing.\n",
    "            For converting the data into vectors we make use of some predefined libraries in python.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Till now we have understood about tokenizing,        stemming, and lemmatizing.',\n",
       " 'All of these are the part of the text cleaning, now after cleaning the text we need to convert the text into some kind of numerical representation called vectors so that we can feed the data to a machine learning model for further processing.',\n",
       " 'For converting the data into vectors we make use of some predefined libraries in python.']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataset)):\n",
    "    dataset[i] = dataset[i].lower()\n",
    "    dataset[i] = re.sub(r'\\W',\" \",dataset[i])\n",
    "    dataset[i] = re.sub(r'\\s+',\" \",dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['till now we have understood about tokenizing stemming and lemmatizing ',\n",
       " 'all of these are the part of the text cleaning now after cleaning the text we need to convert the text into some kind of numerical representation called vectors so that we can feed the data to a machine learning model for further processing ',\n",
       " 'for converting the data into vectors we make use of some predefined libraries in python ']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcnt = {}\n",
    "for data in dataset:\n",
    "    words = nltk.word_tokenize(data)\n",
    "    for word in words:\n",
    "        if word not in wordcnt.keys():\n",
    "            wordcnt[word] = 1\n",
    "        else:\n",
    "            wordcnt[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1,\n",
       " 'about': 1,\n",
       " 'after': 1,\n",
       " 'all': 1,\n",
       " 'and': 1,\n",
       " 'are': 1,\n",
       " 'called': 1,\n",
       " 'can': 1,\n",
       " 'cleaning': 2,\n",
       " 'convert': 1,\n",
       " 'converting': 1,\n",
       " 'data': 2,\n",
       " 'feed': 1,\n",
       " 'for': 2,\n",
       " 'further': 1,\n",
       " 'have': 1,\n",
       " 'in': 1,\n",
       " 'into': 2,\n",
       " 'kind': 1,\n",
       " 'learning': 1,\n",
       " 'lemmatizing': 1,\n",
       " 'libraries': 1,\n",
       " 'machine': 1,\n",
       " 'make': 1,\n",
       " 'model': 1,\n",
       " 'need': 1,\n",
       " 'now': 2,\n",
       " 'numerical': 1,\n",
       " 'of': 4,\n",
       " 'part': 1,\n",
       " 'predefined': 1,\n",
       " 'processing': 1,\n",
       " 'python': 1,\n",
       " 'representation': 1,\n",
       " 'so': 1,\n",
       " 'some': 2,\n",
       " 'stemming': 1,\n",
       " 'text': 3,\n",
       " 'that': 1,\n",
       " 'the': 6,\n",
       " 'these': 1,\n",
       " 'till': 1,\n",
       " 'to': 2,\n",
       " 'tokenizing': 1,\n",
       " 'understood': 1,\n",
       " 'use': 1,\n",
       " 'vectors': 2,\n",
       " 'we': 4}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordcnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in dataset:\n",
    "    vector = list()\n",
    "    for word in wordcnt:\n",
    "        if word in nltk.word_tokenize(data): vector.append(1)\n",
    "        else: vector.append(0)\n",
    "    x.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0],\n",
       "       [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,\n",
       "        1, 1, 1, 1]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = pd.DataFrame(['he is a good boy','she is a good girl', 'boy and girl are good'],columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,sent.shape[0]):\n",
    "    words = sent['text'][i]\n",
    "    words = word_tokenize(words)\n",
    "    texts = [lemmaztier.lemmatize(w) for w in words if w not in stopwords.words('english')]\n",
    "    text = ' '.join(texts)\n",
    "    corpus.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good boy', 'good girl', 'boy girl good']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1],\n",
       "       [0, 1, 1],\n",
       "       [1, 1, 1]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF(Term frequency) & IDF (Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF - Term Frequency\n",
    "\n",
    "IDF - Inverse Document Frequency\n",
    "\n",
    "TF-IDF = TF * IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Term Frequency (TF) = Number of occurrences of a word in a document / Number of words in that document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inverse Document (IDF) = log_e (Total Number of documents / Number of documents containing word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1 (tags/v3.8.1:1b293b6, Dec 18 2019, 22:39:24) [MSC v.1916 32 bit (Intel)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "07d792be6b57be6706d062b64748ace5f7e7b33f6afbc78bdfd04c27fd7929d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
